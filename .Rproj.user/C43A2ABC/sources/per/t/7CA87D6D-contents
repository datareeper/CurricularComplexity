library(dplyr)
library(klaR)
library(tm)
library(SnowballC)
library(stringr)
library(knitr)
library(irrCAC)
data <- read.csv("C:/Users/reepi/Dropbox/Articles/Med Ed/Goal Orientation Data.csv", stringsAsFactors = FALSE)
data <- data[-1,1:9]
data[is.na(data) == TRUE] <- 0
data <- data[,-3]
names(data) <- c("Text", "Group", "Preparedness","Practice","Helped","Growth","Barriers","Experience")

data$Text <- str_replace(data$Text,"  ","")
data$Text <- str_replace(data$Text,"?","")
data$Text <- str_replace(data$Text,"?","")
sum(str_count(data$Text, "?"))

counts <- colSums(data[,4:8])

corpus <- SimpleCorpus(VectorSource(data$Text))
# 1. Stripping any extra white space:
corpus <- tm_map(corpus, stripWhitespace)
# 2. Transforming everything to lowercase
corpus <- tm_map(corpus, content_transformer(tolower))
# 3. Stem documents
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))

dtm <- DocumentTermMatrix(corpus)
words_frequency <- colSums(as.matrix(dtm))
length(words_frequency)

ord <- order(words_frequency, decreasing=TRUE)

# get the top 10 words by frequency of appeearance
words_frequency[head(ord, 10)] %>%
  kable()




DocumentTermMatrix(corpus, control = list(weighting = weightTfIdf)) -> dtm.tfidf

dtm.df <- as.matrix(dtm.tfidf)

similarity <- matrix(NA, nrow=nrow(dtm.df), ncol=nrow(dtm.df))

index2 <- 1
for (index1 in 1:nrow(dtm.df))
{
  while (index2 < nrow(dtm.df))
  {
    similarity[index1,index2] <- cosinesim(dtm.df[index2,],dtm.df[index1,])
    index2 <- index2 + 1
  }
  index2 <- 1
}

dissimilarity <- as.dist(1-similarity)
clustering <- hclust(dissimilarity, method = "ward.D2", members = NULL)
plot(clustering, labels = FALSE)
groups <- cutree(clustering,5)

data$Cluster <- groups

summary.codes <- matrix(0, nrow = 4, ncol = 5)

while (index <= 5)
{
  rows <- which(data$Cluster==index)
  summary.codes[,index] <- colSums(data[rows,4:8])/length(rows)
  index <- index + 1
}


#Uncoded segs
data <- read.csv("C:/Users/reepi/Dropbox/Articles/Med Ed/Clustered groups for GO.csv", stringsAsFactors = FALSE)
data <- data[,-9]

cosinesim <- function(x,y)
{
  x <- as.matrix(x)
  y <- as.matrix(y)
  z <- matrix(data = NA, nrow = length(x))
  for (index in 1:length(x))
  {
    z[index] <- x[index]*y[index]
  }
  lengthx <- sqrt(sum(x^2))
  lengthy <- sqrt(sum(y^2))
  measure <- sum(z)/(lengthx*lengthy)
  measure <- ifelse(is.nan(measure),0.00,measure)
  measure <- as.numeric(format(round(measure, 2), nsmall = 2))
  return(measure)
}

coded <- data
uncoded <- which(rowSums(data[,4:8])==0)
coded <- coded[-uncoded,]

corpus <- SimpleCorpus(VectorSource(coded$Text))
# 1. Stripping any extra white space:
corpus <- tm_map(corpus, stripWhitespace)
# 2. Transforming everything to lowercase
corpus <- tm_map(corpus, content_transformer(tolower))
# 3. Stem documents
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))

dtm <- DocumentTermMatrix(corpus)

DocumentTermMatrix(corpus, control = list(weighting = weightTfIdf)) -> dtm.tfidf

dtm.df <- as.matrix(dtm.tfidf)

similarity <- matrix(NA, nrow=nrow(dtm.df), ncol=nrow(dtm.df))

index2 <- 1
for (index1 in 1:nrow(dtm.df))
{
  while (index2 < nrow(dtm.df))
  {
    similarity[index1,index2] <- cosinesim(dtm.df[index2,],dtm.df[index1,])
    index2 <- index2 + 1
  }
  index2 <- 1
}

dissimilarity <- as.dist(1-similarity)
clustering <- hclust(dissimilarity, method = "ward.D2", members = NULL)
plot(clustering, labels = FALSE)
groups <- cutree(clustering,h = 1.97)  # 2.075
abline(h = 1.965)

coded$Cluster <- groups

summary.codes <- matrix(0, nrow = 7, ncol = 5)

index <- 1
while (index <= 7)
{
  rows <- which(coded$Cluster==index)
  perc <- unname(colSums(coded[rows,4:8])/length(rows))
  summary.codes[index,] <- perc
  index <- index + 1
}

summary.codes <- as.data.frame(summary.codes)
names(summary.codes) <- c("Preparedness","Practice","Helped","Growth","Barriers")


group1 <- filter(coded, Cluster == 1)
group2 <- filter(coded, Cluster == 2)
group3 <- filter(coded, Cluster == 3)
group4 <- filter(coded, Cluster == 4)
group5 <- filter(coded, Cluster == 5)
group6 <- filter(coded, Cluster == 6)
group7 <- filter(coded, Cluster == 7)


summary.codes
coded$Cluster

write.csv(coded, "Clustered groups for GO.csv")
write.csv(summary.codes, "Percent frequency of codes by cluster.csv")




clustered <- read.csv("C:/Users/reepi/Dropbox/Articles/Med Ed/Clustered groups for GO.csv", stringsAsFactors = FALSE)
corpus <- SimpleCorpus(VectorSource(clustered$Text))
# 1. Stripping any extra white space:
corpus <- tm_map(corpus, stripWhitespace)
# 2. Transforming everything to lowercase
corpus <- tm_map(corpus, content_transformer(tolower))
# 3. Stem documents
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
DTM <- DocumentTermMatrix(corpus, control = list("weightTf"))
DTM <- as.matrix(DTM)
DTM <- as.data.frame(cbind(clustered$Cluster,DTM))
names(DTM)[1] <- "Cluster"

group1 <- filter(DTM, Cluster == 1)
group2 <- filter(DTM, Cluster == 2)
group3 <- filter(DTM, Cluster == 3)
group4 <- filter(DTM, Cluster == 4)
group5 <- filter(DTM, Cluster == 5)
group6 <- filter(DTM, Cluster == 6)
group7 <- filter(DTM, Cluster == 7)

stems1 <- sort(colSums(group1), decreasing = TRUE)
names(which(stems1 > 5))
stems2 <- sort(colSums(group2), decreasing = TRUE)
names(which(stems2 > 5))
stems3 <- sort(colSums(group3), decreasing = TRUE)
names(which(stems3 > 5))
stems4 <- sort(colSums(group4), decreasing = TRUE)
names(which(stems4 > 5))
stems5 <- sort(colSums(group5), decreasing = TRUE)
names(which(stems5 > 5))
stems6 <- sort(colSums(group6), decreasing = TRUE)
names(which(stems6 > 5))
stems7 <- sort(colSums(group7), decreasing = TRUE)
names(which(stems7 > 5))


